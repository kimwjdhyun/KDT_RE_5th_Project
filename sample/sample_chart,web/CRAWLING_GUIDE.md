# ì›¹ í¬ë¡¤ë§ ì™„ì „ ê°€ì´ë“œ ğŸ“š

## ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install selenium beautifulsoup4 requests pandas

# í¬ë¡¬ ë“œë¼ì´ë²„ ìë™ ê´€ë¦¬
pip install webdriver-manager

# ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ (ì„ íƒ)
pip install openpyxl

# ëª¨ë‘ í•œ ë²ˆì— ì„¤ì¹˜
pip install selenium beautifulsoup4 requests pandas webdriver-manager openpyxl
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: ê¸°ë³¸ í¬ë¡¤ë§ ì‹¤í–‰

```bash
python data_crawler.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- âœ… ê³µê³µë°ì´í„° í¬í„¸ ê²€ìƒ‰
- âœ… ê¸°ìƒì²­ í˜ì´ì§€ ì ‘ì†
- âœ… í•œêµ­ì—ë„ˆì§€ê³µë‹¨ í†µê³„ ë©”ë‰´ í™•ì¸
- âœ… ì˜ˆì‹œ ë°ì´í„° ìƒì„± ë° ì €ì¥

### 2ë‹¨ê³„: ê³ ê¸‰ í¬ë¡¤ë§ ì‹¤í–‰

```bash
python advanced_crawler.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- âœ… ë™ì  í˜ì´ì§€ ì²˜ë¦¬
- âœ… í…Œì´ë¸” ë°ì´í„° ìë™ ì¶”ì¶œ
- âœ… í˜ì´ì§€ë„¤ì´ì…˜ ìë™ ì²˜ë¦¬
- âœ… ë°ì´í„° ì •í•©ì„± ê²€ì¦

---

## ğŸ”§ ì„¤ì • íŒŒì¼ ìˆ˜ì •

### API í‚¤ ì„¤ì • (`crawler_config.py`)

```python
API_KEYS = {
    'WEATHER_API_KEY': 'ì—¬ê¸°ì—_ë°œê¸‰ë°›ì€_í‚¤_ì…ë ¥',
    'PUBLIC_DATA_API_KEY': 'ì—¬ê¸°ì—_ë°œê¸‰ë°›ì€_í‚¤_ì…ë ¥',
}
```

### API í‚¤ ë°œê¸‰ ë°©ë²•

#### 1. ê¸°ìƒì²­ API
1. https://data.kma.go.kr ì ‘ì†
2. íšŒì›ê°€ì… í›„ ë¡œê·¸ì¸
3. ìƒë‹¨ ë©”ë‰´ â†’ API â†’ ì˜¤í”ˆAPI ì‹ ì²­
4. í™œìš© ëª©ì  ì‘ì„± í›„ ì‹ ì²­
5. ìŠ¹ì¸ í›„ API í‚¤ ë°œê¸‰

#### 2. ê³µê³µë°ì´í„° í¬í„¸ API
1. https://www.data.go.kr ì ‘ì†
2. íšŒì›ê°€ì… í›„ ë¡œê·¸ì¸
3. ë°ì´í„° ê²€ìƒ‰ â†’ "ì‹ ì¬ìƒì—ë„ˆì§€" ê²€ìƒ‰
4. ì›í•˜ëŠ” ë°ì´í„°ì…‹ ì„ íƒ â†’ "í™œìš©ì‹ ì²­" í´ë¦­
5. ìŠ¹ì¸ í›„ ì¸ì¦í‚¤(API í‚¤) ë°œê¸‰

---

## ğŸ’¡ í¬ë¡¤ë§ ê¸°ë²•ë³„ ì‚¬ìš© ì˜ˆì‹œ

### 1. Seleniumìœ¼ë¡œ ë™ì  í˜ì´ì§€ í¬ë¡¤ë§

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# ë“œë¼ì´ë²„ ì´ˆê¸°í™”
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

# í˜ì´ì§€ ì—´ê¸°
driver.get("https://example.com")

# ìš”ì†Œ ì°¾ê¸°
element = driver.find_element(By.ID, "search-box")
element.send_keys("ê°•ì›ë„ ì‹ ì¬ìƒì—ë„ˆì§€")

# ë²„íŠ¼ í´ë¦­
button = driver.find_element(By.CSS_SELECTOR, "button.search")
button.click()

# ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
results = driver.find_elements(By.CLASS_NAME, "result-item")
for result in results:
    print(result.text)

# ì¢…ë£Œ
driver.quit()
```

### 2. BeautifulSoupìœ¼ë¡œ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§

```python
import requests
from bs4 import BeautifulSoup

# í˜ì´ì§€ ìš”ì²­
url = "https://example.com"
response = requests.get(url)

# HTML íŒŒì‹±
soup = BeautifulSoup(response.text, 'html.parser')

# ë°ì´í„° ì¶”ì¶œ
title = soup.find('h1', class_='title').text
items = soup.find_all('div', class_='item')

for item in items:
    name = item.find('span', class_='name').text
    value = item.find('span', class_='value').text
    print(f"{name}: {value}")
```

### 3. í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ

```python
import pandas as pd

# HTML í…Œì´ë¸”ì„ ë°”ë¡œ DataFrameìœ¼ë¡œ
url = "https://example.com/table-page"
tables = pd.read_html(url)

# ì²« ë²ˆì§¸ í…Œì´ë¸”
df = tables[0]
print(df.head())

# CSVë¡œ ì €ì¥
df.to_csv('table_data.csv', index=False, encoding='utf-8-sig')
```

### 4. API ìš”ì²­ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘

```python
import requests
import pandas as pd

# API ì—”ë“œí¬ì¸íŠ¸
url = "http://apis.data.go.kr/your-api-endpoint"

# íŒŒë¼ë¯¸í„° ì„¤ì •
params = {
    'serviceKey': 'YOUR_API_KEY',
    'numOfRows': 100,
    'pageNo': 1,
    'dataType': 'JSON'
}

# ìš”ì²­
response = requests.get(url, params=params)
data = response.json()

# DataFrameìœ¼ë¡œ ë³€í™˜
items = data['response']['body']['items']['item']
df = pd.DataFrame(items)
```

---

## ğŸ¯ ì‹¤ì „ í¬ë¡¤ë§ í”„ë¡œì íŠ¸

### ê°•ì›ë„ ì‹ ì¬ìƒ ì—ë„ˆì§€ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤

```python
from advanced_crawler import AdvancedCrawler
import pandas as pd

# 1. í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
crawler = AdvancedCrawler(headless=False)  # ë¸Œë¼ìš°ì € ë³´ê¸°

try:
    # 2. í•œêµ­ì—ë„ˆì§€ê³µë‹¨ í†µê³„ í˜ì´ì§€
    crawler.driver.get("https://www.knrec.or.kr/biz/statistics/stts/list.do")
    
    # 3. ê°•ì›ë„ ì„ íƒ (ì˜ˆì‹œ)
    region_select = crawler.driver.find_element(By.ID, "region")
    region_select.click()
    
    gangwon = crawler.driver.find_element(By.XPATH, "//option[text()='ê°•ì›']")
    gangwon.click()
    
    # 4. ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
    search_btn = crawler.driver.find_element(By.CSS_SELECTOR, "button.search")
    search_btn.click()
    
    # 5. ê²°ê³¼ í…Œì´ë¸” ì¶”ì¶œ
    df = crawler.extract_table_data()
    
    # 6. ë°ì´í„° ì •ì œ
    df = crawler.validate_data(df)
    
    # 7. ì €ì¥
    df.to_csv('gangwon_energy_real.csv', index=False, encoding='utf-8-sig')
    print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    
finally:
    crawler.close()
```

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### 1. ChromeDriver ì˜¤ë¥˜

**ë¬¸ì œ:** `selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH`

**í•´ê²°:**
```bash
pip install webdriver-manager
```

ì½”ë“œì—ì„œ:
```python
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
```

### 2. í•œê¸€ ê¹¨ì§

**ë¬¸ì œ:** CSV íŒŒì¼ì—ì„œ í•œê¸€ì´ ê¹¨ì ¸ ë³´ì„

**í•´ê²°:**
```python
df.to_csv('data.csv', encoding='utf-8-sig', index=False)  # utf-8-sig ì‚¬ìš©
```

### 3. ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ë¬¸ì œ:** `NoSuchElementException`

**í•´ê²°:**
```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.ID, "element-id"))
)
```

### 4. í˜ì´ì§€ ë¡œë”©ì´ ëŠë¦¼

**ë¬¸ì œ:** í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë˜ê¸° ì „ì— í¬ë¡¤ë§ ì‹œë„

**í•´ê²°:**
```python
import time

driver.get(url)
time.sleep(3)  # 3ì´ˆ ëŒ€ê¸°

# ë˜ëŠ” ëª…ì‹œì  ëŒ€ê¸°
driver.implicitly_wait(10)  # ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
```

### 5. ë´‡ íƒì§€ë¡œ ì°¨ë‹¨

**ë¬¸ì œ:** `403 Forbidden` ë˜ëŠ” captcha ë°œìƒ

**í•´ê²°:**
```python
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
options.add_argument('--disable-blink-features=AutomationControlled')

driver = webdriver.Chrome(options=options)
```

---

## ğŸ“Š ë°ì´í„° í›„ì²˜ë¦¬

### ê²°ì¸¡ì¹˜ ì²˜ë¦¬

```python
# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())

# ê²°ì¸¡ì¹˜ ì œê±°
df_clean = df.dropna()

# ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
df['ì»¬ëŸ¼ëª…'].fillna(df['ì»¬ëŸ¼ëª…'].mean(), inplace=True)  # í‰ê· ê°’ìœ¼ë¡œ
df['ì»¬ëŸ¼ëª…'].fillna(method='ffill', inplace=True)  # ì• ê°’ìœ¼ë¡œ
```

### ì¤‘ë³µ ì œê±°

```python
# ì¤‘ë³µ í™•ì¸
print(f"ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")

# ì¤‘ë³µ ì œê±°
df_unique = df.drop_duplicates()
```

### ë°ì´í„° íƒ€ì… ë³€í™˜

```python
# ë¬¸ìì—´ â†’ ìˆ«ì
df['ë°œì „ëŸ‰'] = pd.to_numeric(df['ë°œì „ëŸ‰'], errors='coerce')

# ë¬¸ìì—´ â†’ ë‚ ì§œ
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])

# ì‰¼í‘œ ì œê±° í›„ ìˆ«ì ë³€í™˜
df['ê°’'] = df['ê°’'].str.replace(',', '').astype(float)
```

---

## ğŸ“ í•™ìŠµ ë¦¬ì†ŒìŠ¤

### Selenium ê³µì‹ ë¬¸ì„œ
- https://www.selenium.dev/documentation/

### BeautifulSoup ê³µì‹ ë¬¸ì„œ
- https://www.crummy.com/software/BeautifulSoup/bs4/doc/

### í¬ë¡¤ë§ ì—°ìŠµ ì‚¬ì´íŠ¸
- http://quotes.toscrape.com (ì´ˆë³´ììš©)
- https://books.toscrape.com (ì‹¤ìŠµìš©)

### í•œêµ­ ê³µê³µë°ì´í„° í¬í„¸
- https://www.data.go.kr
- https://kosis.kr
- https://data.kma.go.kr

---

## âš–ï¸ ì›¹ í¬ë¡¤ë§ ì£¼ì˜ì‚¬í•­

### ë²•ì  ì£¼ì˜ì‚¬í•­
1. **robots.txt í™•ì¸**: ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ì •ì±… ì¤€ìˆ˜
2. **ì €ì‘ê¶Œ**: ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ì €ì‘ê¶Œ í™•ì¸
3. **ê°œì¸ì •ë³´**: ê°œì¸ì •ë³´ ìˆ˜ì§‘ ê¸ˆì§€
4. **ìƒì—…ì  ì´ìš©**: ìƒì—…ì  ëª©ì  ì‹œ ë³„ë„ í—ˆê°€ í•„ìš”

### ê¸°ìˆ ì  ì£¼ì˜ì‚¬í•­
1. **ìš”ì²­ ê°„ê²©**: ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì ì ˆí•œ ëŒ€ê¸° ì‹œê°„ ì„¤ì •
2. **User-Agent**: ë´‡ì´ ì•„ë‹˜ì„ ë‚˜íƒ€ë‚´ëŠ” í—¤ë” ì¶”ê°€
3. **ì—ëŸ¬ ì²˜ë¦¬**: try-exceptë¡œ ì˜¤ë¥˜ ì²˜ë¦¬
4. **ë¡œê¹…**: í¬ë¡¤ë§ ê³¼ì • ê¸°ë¡

### ì˜ˆì‹œ ì½”ë“œ

```python
import time
import random

# 1. ì ì ˆí•œ ëŒ€ê¸° ì‹œê°„
time.sleep(random.uniform(2, 5))  # 2~5ì´ˆ ëœë¤ ëŒ€ê¸°

# 2. User-Agent ì„¤ì •
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# 3. ì—ëŸ¬ ì²˜ë¦¬
try:
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")

# 4. ë¡œê¹…
import logging
logging.basicConfig(level=logging.INFO)
logging.info("í¬ë¡¤ë§ ì‹œì‘")
```

---

## ğŸ“ ì‹¤ìŠµ ê³¼ì œ

### ì´ˆê¸‰: ë‹¨ìˆœ í˜ì´ì§€ í¬ë¡¤ë§
- ê°•ì›ë„ì²­ í™ˆí˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ì œëª© ê°€ì ¸ì˜¤ê¸°

### ì¤‘ê¸‰: í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
- KOSISì—ì„œ ê°•ì›ë„ ì¸êµ¬ í†µê³„ í…Œì´ë¸” í¬ë¡¤ë§

### ê³ ê¸‰: ë™ì  í˜ì´ì§€ + API
- ê³µê³µë°ì´í„° í¬í„¸ì—ì„œ ê²€ìƒ‰ â†’ API í™œìš© â†’ ë°ì´í„° í†µí•©

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. ì‹¤ì œ API í‚¤ ë°œê¸‰ë°›ê¸°
2. í¬ë¡¤ë§ ìë™í™” ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬ì¶•
3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì œì‘

---

**Happy Crawling! ğŸ•·ï¸**
