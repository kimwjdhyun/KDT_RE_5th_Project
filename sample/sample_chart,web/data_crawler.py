"""
ê°•ì›ë„ ì‹ ì¬ìƒ ì—ë„ˆì§€ ë°ì´í„° í¬ë¡¤ë§
- ê³µê³µë°ì´í„° í¬í„¸
- í•œêµ­ì—ë„ˆì§€ê³µë‹¨
- ê¸°ìƒì²­ ê¸°ìƒìë£Œê°œë°©í¬í„¸
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time
import requests
import json

print("="*60)
print("ê°•ì›ë„ ì‹ ì¬ìƒ ì—ë„ˆì§€ ë°ì´í„° í¬ë¡¤ë§")
print("="*60)

# ==================== ì„¤ì • ====================
# Chrome ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

print("\n[1] ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")

# ì£¼ì˜: ChromeDriver ê²½ë¡œë¥¼ ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
# ë˜ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” webdriver-manager ì‚¬ìš©
try:
    from webdriver_manager.chrome import ChromeDriverManager
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    print("âœ… ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ (webdriver-manager ì‚¬ìš©)")
except ImportError:
    print("âš ï¸ webdriver-managerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install webdriver-manager")
    # ìˆ˜ë™ ê²½ë¡œ ì§€ì • (ì˜ˆì‹œ)
    # driver = webdriver.Chrome(service=Service('C:/chromedriver/chromedriver.exe'), options=chrome_options)
    print("   ë˜ëŠ” ChromeDriverë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
    exit(1)

# ==================== 1. ê³µê³µë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ====================
def crawl_public_data():
    """
    ê³µê³µë°ì´í„° í¬í„¸ì—ì„œ ì‹ ì¬ìƒ ì—ë„ˆì§€ ë°œì „ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
    """
    print("\n[2] ê³µê³µë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        url = "https://www.data.go.kr/index.do"
        driver.get(url)
        time.sleep(3)
        
        # ê²€ìƒ‰ì°½ ì°¾ê¸°
        search_box = driver.find_element(By.ID, "header-query")
        search_box.clear()
        search_box.send_keys("ê°•ì›ë„ ì‹ ì¬ìƒì—ë„ˆì§€ ë°œì „ëŸ‰")
        
        # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        search_btn = driver.find_element(By.CSS_SELECTOR, "button.btn-search")
        search_btn.click()
        time.sleep(3)
        
        # ê²°ê³¼ í˜ì´ì§€ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ë°ì´í„°ì…‹ ëª©ë¡ ì¶”ì¶œ (ì˜ˆì‹œ)
        datasets = []
        result_items = soup.find_all('div', class_='result-item')
        
        for item in result_items[:5]:  # ìƒìœ„ 5ê°œë§Œ
            try:
                title = item.find('a', class_='title').text.strip()
                org = item.find('span', class_='org').text.strip() if item.find('span', 'org') else 'N/A'
                datasets.append({
                    'ì œëª©': title,
                    'ì œê³µê¸°ê´€': org
                })
            except:
                continue
        
        df_datasets = pd.DataFrame(datasets)
        print(f"âœ… ë°ì´í„°ì…‹ {len(datasets)}ê°œ ë°œê²¬")
        print(df_datasets)
        
        return df_datasets
        
    except Exception as e:
        print(f"âŒ ê³µê³µë°ì´í„° í¬í„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ==================== 2. ê¸°ìƒì²­ ë°ì´í„° í¬ë¡¤ë§ ====================
def crawl_weather_data():
    """
    ê¸°ìƒì²­ ê¸°ìƒìë£Œê°œë°©í¬í„¸ì—ì„œ ê°•ì›ë„ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘
    ì‹¤ì œë¡œëŠ” API í‚¤ê°€ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì›¹ í¬ë¡¤ë§ ì˜ˆì‹œ
    """
    print("\n[3] ê¸°ìƒì²­ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        # ê¸°ìƒìë£Œê°œë°©í¬í„¸
        url = "https://data.kma.go.kr/climate/RankState/selectRankStatisticsDivisionList.do?pgmNo=179"
        driver.get(url)
        time.sleep(3)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # í˜ì´ì§€ ì œëª© í™•ì¸
        page_title = soup.find('title')
        print(f"âœ… í˜ì´ì§€ ì ‘ì† ì™„ë£Œ: {page_title.text if page_title else 'N/A'}")
        
        # ì‹¤ì œ ë°ì´í„°ëŠ” ë¡œê·¸ì¸ì´ë‚˜ API í‚¤ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
        print("ğŸ’¡ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ì„œëŠ” ê¸°ìƒì²­ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   ë°œê¸‰: https://data.kma.go.kr/api/selectApiList.do")
        
        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
        weather_data = {
            'ì§€ì—­': ['ì¶˜ì²œ', 'ê°•ë¦‰', 'ì›ì£¼', 'ì†ì´ˆ', 'íƒœë°±'],
            'ê°•ìˆ˜ëŸ‰(mm)': [1245, 1398, 1189, 1423, 1456],
            'ì¼ì¡°ì‹œê°„(hr)': [2156, 2401, 2234, 2278, 2089],
            'í‰ê· í’ì†(m/s)': [2.3, 3.8, 2.1, 3.2, 4.2]
        }
        df_weather = pd.DataFrame(weather_data)
        print(f"âœ… ê¸°ìƒ ë°ì´í„° {len(df_weather)}ê°œ ì§€ì—­")
        print(df_weather)
        
        return df_weather
        
    except Exception as e:
        print(f"âŒ ê¸°ìƒì²­ ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ==================== 3. í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ë°ì´í„° í¬ë¡¤ë§ ====================
def crawl_energy_data():
    """
    í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ì‹ ì¬ìƒì—ë„ˆì§€ì„¼í„°ì—ì„œ ë°œì „ëŸ‰ í†µê³„ ìˆ˜ì§‘
    """
    print("\n[4] í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        url = "https://www.knrec.or.kr/biz/statistics/stts/list.do"
        driver.get(url)
        time.sleep(3)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        print("âœ… í•œêµ­ì—ë„ˆì§€ê³µë‹¨ í˜ì´ì§€ ì ‘ì† ì™„ë£Œ")
        print("ğŸ’¡ í†µê³„ ë°ì´í„°ëŠ” ë¡œê·¸ì¸ ë˜ëŠ” ë³„ë„ ì‹ ì²­ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì˜ˆì‹œ: í˜ì´ì§€ì—ì„œ í†µê³„ ë©”ë‰´ ì°¾ê¸°
        menu_items = soup.find_all('a', href=True)
        energy_links = [item for item in menu_items if 'í†µê³„' in item.text or 'ë°œì „ëŸ‰' in item.text]
        
        print(f"âœ… ê´€ë ¨ ë©”ë‰´ {len(energy_links)}ê°œ ë°œê²¬")
        for i, link in enumerate(energy_links[:5]):
            print(f"   {i+1}. {link.text.strip()}")
        
        # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
        energy_data = {
            'ì—°ë„': [2019, 2020, 2021, 2022, 2023, 2024],
            'íƒœì–‘ê´‘(GWh)': [1245, 1456, 1678, 1923, 2187, 2456],
            'í’ë ¥(GWh)': [892, 1023, 1234, 1456, 1678, 1892],
            'ìˆ˜ë ¥(GWh)': [1567, 1623, 1598, 1634, 1672, 1701]
        }
        df_energy = pd.DataFrame(energy_data)
        print("âœ… ì—°ë„ë³„ ë°œì „ëŸ‰ ë°ì´í„°")
        print(df_energy)
        
        return df_energy
        
    except Exception as e:
        print(f"âŒ í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ==================== 4. BeautifulSoupìœ¼ë¡œ ê°„ë‹¨í•œ HTML íŒŒì‹± ====================
def crawl_simple_page(url, keyword):
    """
    BeautifulSoupì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ í˜ì´ì§€ í¬ë¡¤ë§
    """
    print(f"\n[5] BeautifulSoupìœ¼ë¡œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # íƒ€ì´í‹€ ì¶”ì¶œ
        title = soup.find('title')
        print(f"âœ… í˜ì´ì§€ ì œëª©: {title.text if title else 'N/A'}")
        
        # í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
        text_elements = soup.find_all(string=lambda text: keyword in text if text else False)
        print(f"âœ… '{keyword}' í¬í•¨ëœ ìš”ì†Œ {len(text_elements)}ê°œ ë°œê²¬")
        
        return soup
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None

# ==================== 5. ë°ì´í„° ì €ì¥ ====================
def save_data(df, filename, file_format='csv'):
    """
    ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    """
    try:
        if file_format == 'csv':
            df.to_csv(f'C:/Users/dkreh/Desktop/KDT_RE_5th/3_Project/{filename}.csv', 
                     index=False, encoding='utf-8-sig')
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}.csv")
        elif file_format == 'excel':
            df.to_excel(f'C:/Users/dkreh/Desktop/KDT_RE_5th/3_Project/{filename}.xlsx', 
                       index=False, engine='openpyxl')
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}.xlsx")
        elif file_format == 'json':
            df.to_json(f'C:/Users/dkreh/Desktop/KDT_RE_5th/3_Project/{filename}.json', 
                      orient='records', force_ascii=False, indent=2)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}.json")
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==================== ì‹¤í–‰ ====================
if __name__ == "__main__":
    try:
        # 1. ê³µê³µë°ì´í„° í¬í„¸
        df_datasets = crawl_public_data()
        if not df_datasets.empty:
            save_data(df_datasets, 'datasets_list', 'csv')
        
        # 2. ê¸°ìƒì²­ ë°ì´í„°
        df_weather = crawl_weather_data()
        if not df_weather.empty:
            save_data(df_weather, 'weather_data', 'csv')
        
        # 3. í•œêµ­ì—ë„ˆì§€ê³µë‹¨
        df_energy = crawl_energy_data()
        if not df_energy.empty:
            save_data(df_energy, 'energy_data', 'csv')
        
        # 4. ê°•ì›ë„ì²­ í˜ì´ì§€ í¬ë¡¤ë§ (BeautifulSoup)
        gangwon_url = "https://state.gwd.go.kr/portal"
        crawl_simple_page(gangwon_url, "ì‹ ì¬ìƒ")
        
        print("\n" + "="*60)
        print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print("   - datasets_list.csv (ê³µê³µë°ì´í„° ëª©ë¡)")
        print("   - weather_data.csv (ê¸°ìƒ ë°ì´í„°)")
        print("   - energy_data.csv (ë°œì „ëŸ‰ ë°ì´í„°)")
        print("\nğŸ’¡ Tip:")
        print("   - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ì„œëŠ” API í‚¤ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("   - ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ ì½”ë“œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤")
        print("   - í¬ë¡¤ë§ ê°„ê²©(time.sleep)ì„ ì ì ˆíˆ ì¡°ì ˆí•˜ì„¸ìš”")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        driver.quit()
        print("\nâœ… ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")

print("\n" + "="*60)
