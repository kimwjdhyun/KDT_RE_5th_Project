"""
ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ê¸°ë²•
- ë™ì  í˜ì´ì§€ ì²˜ë¦¬
- í˜ì´ì§€ë„¤ì´ì…˜ ìë™ ì²˜ë¦¬
- ë°ì´í„° ì •í•©ì„± ê²€ì¦
- ê²°ì¸¡ì¹˜ ì²˜ë¦¬
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re

class AdvancedCrawler:
    """ê³ ê¸‰ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, headless=True):
        """ì´ˆê¸°í™”"""
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.chrome.service import Service
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
        
        try:
            from webdriver_manager.chrome import ChromeDriverManager
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=chrome_options
            )
            print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def wait_for_element(self, by, value, timeout=10):
        """ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, value))
            )
            return element
        except TimeoutException:
            print(f"âš ï¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {value}")
            return None
    
    def scroll_to_bottom(self, pause_time=2):
        """í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ (ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ìš©)"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(pause_time)
            
            # ìƒˆë¡œìš´ ë†’ì´ ê³„ì‚°
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
            last_height = new_height
        
        print("âœ… í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ì™„ë£Œ")
    
    def crawl_with_pagination(self, base_url, max_pages=5):
        """
        í˜ì´ì§€ë„¤ì´ì…˜ì´ ìˆëŠ” ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        """
        all_data = []
        
        for page in range(1, max_pages + 1):
            try:
                url = f"{base_url}?page={page}"
                self.driver.get(url)
                time.sleep(2)
                
                print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
                
                # ë°ì´í„° ì¶”ì¶œ ë¡œì§
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                items = soup.find_all('div', class_='data-item')  # ì‹¤ì œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ìˆ˜ì •
                
                page_data = []
                for item in items:
                    try:
                        # ë°ì´í„° ì¶”ì¶œ (ì˜ˆì‹œ)
                        title = item.find('h3').text.strip()
                        value = item.find('span', class_='value').text.strip()
                        page_data.append({'ì œëª©': title, 'ê°’': value})
                    except:
                        continue
                
                all_data.extend(page_data)
                print(f"   âœ… {len(page_data)}ê°œ í•­ëª© ìˆ˜ì§‘")
                
            except Exception as e:
                print(f"   âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                break
        
        return pd.DataFrame(all_data)
    
    def extract_table_data(self, table_selector='table'):
        """
        HTML í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        """
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            tables = soup.find_all(table_selector)
            
            if not tables:
                print("âš ï¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return pd.DataFrame()
            
            # ì²« ë²ˆì§¸ í…Œì´ë¸” ì¶”ì¶œ
            table = tables[0]
            
            # í—¤ë” ì¶”ì¶œ
            headers = []
            thead = table.find('thead')
            if thead:
                headers = [th.text.strip() for th in thead.find_all('th')]
            
            # ë°ì´í„° ì¶”ì¶œ
            rows = []
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr'):
                row = [td.text.strip() for td in tr.find_all(['td', 'th'])]
                if row:
                    rows.append(row)
            
            # DataFrame ìƒì„±
            if headers:
                df = pd.DataFrame(rows, columns=headers)
            else:
                df = pd.DataFrame(rows)
            
            print(f"âœ… í…Œì´ë¸” ë°ì´í„° {len(df)}í–‰ ì¶”ì¶œ")
            return df
            
        except Exception as e:
            print(f"âŒ í…Œì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def download_file(self, download_url, save_path):
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        """
        try:
            import requests
            response = requests.get(download_url, timeout=30)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            print(f"âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path}")
            return True
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def clean_text(self, text):
        """
        í…ìŠ¤íŠ¸ ì •ì œ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        """
        if not text:
            return ""
        
        # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r'\s+', ' ', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        # íŠ¹ì • íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
        text = re.sub(r'[^\w\sê°€-í£.,()%-]', '', text)
        
        return text
    
    def validate_data(self, df):
        """
        ë°ì´í„° ì •í•©ì„± ê²€ì¦
        """
        print("\nğŸ“Š ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        # 1. ê²°ì¸¡ì¹˜ í™•ì¸
        missing = df.isnull().sum()
        if missing.any():
            print(f"âš ï¸ ê²°ì¸¡ì¹˜ ë°œê²¬:")
            print(missing[missing > 0])
        else:
            print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        
        # 2. ì¤‘ë³µ í™•ì¸
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            print(f"âš ï¸ ì¤‘ë³µ ë°ì´í„° {duplicates}ê°œ ë°œê²¬")
        else:
            print("âœ… ì¤‘ë³µ ë°ì´í„° ì—†ìŒ")
        
        # 3. ë°ì´í„° íƒ€ì… í™•ì¸
        print("\nğŸ“‹ ë°ì´í„° íƒ€ì…:")
        print(df.dtypes)
        
        # 4. ê¸°ë³¸ í†µê³„
        print("\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(df.describe())
        
        return df
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ")

# ==================== íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ ====================

def crawl_kosis_data(crawler):
    """
    êµ­ê°€í†µê³„í¬í„¸(KOSIS) ë°ì´í„° í¬ë¡¤ë§
    """
    print("\n[KOSIS] êµ­ê°€í†µê³„í¬í„¸ í¬ë¡¤ë§...")
    
    try:
        url = "https://kosis.kr/statHtml/statHtml.do?orgId=388&tblId=DT_388N_0001"
        crawler.driver.get(url)
        time.sleep(3)
        
        # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        df = crawler.extract_table_data()
        
        if not df.empty:
            # ë°ì´í„° ì •ì œ
            df = df.applymap(crawler.clean_text)
            print(f"âœ… KOSIS ë°ì´í„° {len(df)}í–‰ ìˆ˜ì§‘")
            return df
        
    except Exception as e:
        print(f"âŒ KOSIS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    return pd.DataFrame()

def crawl_weather_api_data():
    """
    ê¸°ìƒì²­ APIë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ìˆ˜ì§‘ (API í‚¤ í•„ìš”)
    """
    print("\n[ê¸°ìƒì²­ API] ë°ì´í„° ìˆ˜ì§‘...")
    
    # API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤
    API_KEY = "YOUR_API_KEY_HERE"
    
    if API_KEY == "YOUR_API_KEY_HERE":
        print("âš ï¸ ê¸°ìƒì²­ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        print("   ë°œê¸‰: https://data.kma.go.kr/api/selectApiList.do")
        return pd.DataFrame()
    
    try:
        import requests
        
        # ì˜ˆì‹œ: íŠ¹ì • ì§€ì ì˜ ì¼ë³„ ë°ì´í„°
        url = "http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList"
        
        params = {
            'serviceKey': API_KEY,
            'numOfRows': 100,
            'pageNo': 1,
            'dataCd': 'ASOS',
            'dateCd': 'DAY',
            'startDt': '20240101',
            'endDt': '20241231',
            'stnIds': '101',  # ì¶˜ì²œ
            'dataType': 'JSON'
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        items = data.get('response', {}).get('body', {}).get('items', {}).get('item', [])
        df = pd.DataFrame(items)
        
        print(f"âœ… ê¸°ìƒì²­ API ë°ì´í„° {len(df)}í–‰ ìˆ˜ì§‘")
        return df
        
    except Exception as e:
        print(f"âŒ ê¸°ìƒì²­ API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ==================== ì‹¤í–‰ ì˜ˆì‹œ ====================
if __name__ == "__main__":
    print("="*60)
    print("ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ì‹¤í–‰")
    print("="*60)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = AdvancedCrawler(headless=True)
    
    try:
        # 1. KOSIS ë°ì´í„°
        df_kosis = crawl_kosis_data(crawler)
        if not df_kosis.empty:
            crawler.validate_data(df_kosis)
            df_kosis.to_csv('C:/Users/dkreh/Desktop/KDT_RE_5th/3_Project/kosis_data.csv', 
                           index=False, encoding='utf-8-sig')
        
        # 2. ê¸°ìƒì²­ API ë°ì´í„°
        df_weather = crawl_weather_api_data()
        if not df_weather.empty:
            df_weather.to_csv('C:/Users/dkreh/Desktop/KDT_RE_5th/3_Project/weather_api_data.csv',
                             index=False, encoding='utf-8-sig')
        
        print("\n" + "="*60)
        print("âœ… ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        crawler.close()
